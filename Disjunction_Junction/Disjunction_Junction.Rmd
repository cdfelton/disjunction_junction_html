---
title: "Disjunction Junction"
output: html_document
date: '2022-10-11'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Load required libraries
```{r, include= FALSE}
library('tidyverse')
library('ggplot2')
library('lme4')
library('lmerTest')
library('boot')
library('ggpubr')
```

# Experiment 1 Cleaning and Analysis
Read in and join raw data with the key
```{r}
# Reads in data files
data_1 = read.csv('../Disjunction_Junction/Data&Resources/Real_Data/dis_junct_preliminary-trials.csv')
data_2 = read.csv('../Disjunction_Junction/Data&Resources/Real_Data/dis_junc_full_collection-trials.csv')

# Combines the data from the two csvs
data = rbind(data_1, data_2)

# Factorizes and renames id column
data = data %>% 
  mutate(ID = as.factor(id))

# Select just the columns needed for analysis
data = data %>% select(workerid, ID, chance)

# Reads in key file
key = read.csv('../Disjunction_Junction/Data&Resources/disjunction_junction_key.csv')

# Factorizes IS column
key$ID = as_factor(key$ID)

# Joins key and data
data = left_join(data, key, by = 'ID')
```

# Data Cleaning
There are a total of 6 attention checking control items, each of which should result in either close to 0% or close to 100% chance ratings from participants who were paying attention. The following code block excludes participants who did not rate at least 5 out of the 6 control items as within 10% of either 0% or 100%, whichever was accurate. 
```{r}
#Filter just the 100% control items
cntr_100 = data %>% filter(ID == 48 | ID == 49 | ID == 50)
#Filter just the 0% control items
cntr_0 = data %>% filter(ID == 51 | ID == 52 | ID == 53)

#Adds a column for accuracy
cntr_100$correct = cntr_100$chance >= 90
cntr_0$correct = cntr_0$chance <= 10

#Combines the items
accuracy_check = rbind(cntr_0, cntr_100)

#Creates a table with the aggregated mean accuracy for each participant
accuracy_table = accuracy_check %>% group_by(workerid) %>% summarise(mean_accuracy = mean(correct))

#Join the accuracy table to the rest of the data
data = left_join(data, accuracy_table, by = 'workerid')

#Filter out participants who got less than 5/6 correct (Starting with 50, 8 are excluded, leaving 42)
data = data %>% filter(mean_accuracy > .8)
```

# Overview of Analysis for Experiment One
In the first phase of this experiment, we have 1 main hypothesis to test: Whether the direction of the bias in participant ratings matches the literature's prediction of inclusive or exclusive bias for the overall disjunction. 


The following code aggregates group means, standard deviations, and standard errors for each item
```{r}
means = data %>% 
  group_by(ID) %>% 
  summarise(Mean_Chance = mean(chance),
            Standard_Deviation = sd(chance),
            Standard_Error = Standard_Deviation/sqrt(length(unique(data$workerid))))
print(means)
```

The code below generates a plot with columns for the means and +/_ 1 SE errorbars
```{r}
graph_data = left_join(means, key, by = 'ID') %>% mutate(ID = fct_reorder(ID, Mean_Chance))

graph_data$Claimed.Bias[graph_data$Claimed.Bias == 'control'] = 'Control'
graph_data$Claimed.Bias[graph_data$Claimed.Bias == 'Exclusive Biased'] = 'Hypothesized Exclusive'
graph_data$Claimed.Bias[graph_data$Claimed.Bias == 'Inclusive Biased'] = 'Hypothesized Inclusive'
graph_data$Claimed.Bias[graph_data$Claimed.Bias == 'N/A'] = 'No Prediction Made'

graph_data = rename(graph_data, 'Hypothesized_Reading' = 'Claimed.Bias')

g = ggplot(graph_data, aes(ID, Mean_Chance))
g = g + geom_col(position = "dodge", aes(fill = Hypothesized_Reading)) + labs(title = 'Means and Variation in Compatability Ratings by Item', x = 'Items', y = 'Mean Compatibility Rating (%)')+
  geom_errorbar(width = .5,
                aes(ymin = Mean_Chance - Standard_Error*2, ymax = Mean_Chance + Standard_Error*2),
                position = position_dodge(.9)) +
               theme_bw() +
               theme(axis.text.x = element_blank()) + labs(fill = "Hypothesized Reading")
```

### The next section uses a linear model to test the hypothesis
```{r}
# Filter out items with no recorded bias claim
model_data = data %>% filter(Claimed.Bias == 'Exclusive Biased' | Claimed.Bias == 'Inclusive Biased')

# transform column into factor
model_data$Claimed.Bias = as_factor(model_data$Claimed.Bias)

# An exploratory basic lm()
model_0 = lm(chance ~ Claimed.Bias, data = model_data)
summary(model_0)

# Fixed version of the old model (will actually run, rank deficient when ID is a fixed effect)
model_2 = lmer(chance ~ Claimed.Bias + (1 | workerid), data = model_data)
summary(model_2)

# Bayesian version of the model to conform to new stats practices
model_1 = brm(chance ~ Claimed.Bias + (1 | workerid),
             data = model_data,
             family = gaussian,
             iter = 4000,
             cores = 4,
             chains = 4,
             warmup = 1000,
             # control = list(adapt_delta = 0.99, max_treedepth = 15),
             seed = 4
             )
print(model_1)
plot(model_1)
```

# Experiment 2 Analysis

The following code reads in and joins the data from experiment 2 to its respective key as well as the means data from experiment 1. 
```{r}
# Reads in data files
data_part2 = read.csv('../Disjunction_Junction/Data&Resources/Real_Data/dis_junct_part2.csv')

# Factorizes and renames id column
data_part2 = data_part2 %>% 
  mutate(ID = as.factor(id))

# Select just the columns needed for analysis
data_part2 = data_part2 %>% select(workerid, ID, chance)

# Renames chance to something more easily interpretable for experiment 2
data_part2 = mutate(data_part2, Implicature_Likelihood= data_part2$chance)

# Reads in key file
key2 = read.csv('../Disjunction_Junction/Data&Resources/disjunction_junction_key2.csv')

# Factorizes IS column
key2$ID = as_factor(key2$ID)

# Joins key and data
data_part2 = left_join(data_part2, key2, by = 'ID')

```

# Data Cleaning for Part 2
There are a total of 6 attention checking control items, each of which should result in either close to 0% or close to 100% chance ratings from participants who were paying attention. The following code block excludes participants who did not rate at least 5 out of the 6 control items as within 10% of either 0% or 100%, whichever was accurate.

```{r}
#Filter just the 100% control items
cntr_100_part2 = data_part2 %>% filter(ID == 48 | ID == 49 | ID == 50)
#Filter just the 0% control items
cntr_0_part2 = data_part2 %>% filter(ID == 51 | ID == 52 | ID == 53)

#Adds a column for accuracy
cntr_100_part2$correct = cntr_100_part2$Implicature_Likelihood >= 70
cntr_0_part2$correct = cntr_0_part2$Implicature_Likelihood <= 30

#Combines the items
accuracy_check_part2 = rbind(cntr_0_part2, cntr_100_part2)

#Creates a table with the aggregated mean accuracy for each participant
accuracy_table_part2 = accuracy_check_part2 %>% group_by(workerid) %>% summarise(mean_accuracy = mean(correct))

#Join the accuracy table to the rest of the data
data_part2 = left_join(data_part2, accuracy_table_part2, by = 'workerid')

#Filter out participants who got less than 5/6 correct (Starting with 50, 9 are excluded, leaving 41)
data_part2 = data_part2 %>% filter(mean_accuracy > .8)
```

# Overview of Analysis for Experiment Two
Experiment two aimed to test our second main research question: To what extent do the prior incompatibility ratings from experiment one predict the implicature rates from experiment two?

The following code generates the means and measures of variance for the data from experiment 2. 
```{r}
mean_part2 = data_part2 %>% 
  group_by(ID) %>% 
  summarise(Implicature_Rate = mean(Implicature_Likelihood),
            Standard_Deviation_Imp = sd(Implicature_Likelihood),
            Standard_Error_Imp  = Standard_Deviation_Imp/sqrt(length(unique(data_part2$workerid))))

```

The following code generates a similar plot to the data from part 1
```{r}
graph_data2 = left_join(mean_part2, key2, by = 'ID') 
graph_data2 = left_join(graph_data2, means, by = 'ID') %>% mutate(ID = fct_reorder(ID, Mean_Chance))

graph_data2$Hypothesized_Bias[graph_data2$Hypothesized_Bias == 'control'] = 'Control'
graph_data2$Hypothesized_Bias[graph_data2$Hypothesized_Bias == 'Exclusive Biased'] = 'Hypothesized Exclusive'
graph_data2$Hypothesized_Bias[graph_data2$Hypothesized_Bias == 'Inclusive Biased'] = 'Hypothesized Inclusive'
graph_data2$Hypothesized_Bias[graph_data2$Hypothesized_Bias == 'N/A'] = 'No Prediction Made'

graph_data2 = rename(graph_data2, 'Hypothesized_Reading' = 'Hypothesized_Bias')

g2 = ggplot(graph_data2, aes(ID, Implicature_Rate))
g2 = g2 + geom_col(position = "dodge", aes(fill = Hypothesized_Reading)) + labs(title = 'Means and Variation in Implication Generation by Item', x = 'Items', y = 'Mean Inclusivity Ratings (%)')+
  geom_errorbar(width = .5,
                aes(ymin = Implicature_Rate - Standard_Error_Imp*2, ymax = Implicature_Rate + Standard_Error_Imp*2),
                position = position_dodge(.9)) +
               theme_bw() +
               theme(axis.text.x = element_blank()) + labs(fill = "Hypothesized Reading")
```
The following code graphs the relationship between the data in each experiment.
```{r}
graph3_data = left_join(means, mean_part2, by = "ID") %>% mutate(ID = fct_reorder(ID, Mean_Chance))
graph3_data = left_join(graph3_data, key2, by = 'ID')

graph3_data$Hypothesized_Bias[graph3_data$Hypothesized_Bias == 'control'] = 'Control'
graph3_data$Hypothesized_Bias[graph3_data$Hypothesized_Bias == 'Exclusive Biased'] = 'Hypothesized Exclusive'
graph3_data$Hypothesized_Bias[graph3_data$Hypothesized_Bias == 'Inclusive Biased'] = 'Hypothesized Inclusive'
graph3_data$Hypothesized_Bias[graph3_data$Hypothesized_Bias == 'N/A'] = 'No Prediction Made'

g3 = ggplot(aes(x = Mean_Chance, y = Implicature_Rate), data = graph3_data) + 
  labs(title = 'Correlation of Compatability and Implication Rate', x = 'Mean Compatibility Ratings (%)', y = 'Mean Inclusivity Ratings (%)')+
  geom_point(aes(color = Hypothesized_Bias)) +
  geom_smooth(method = lm, se = FALSE) + labs(fill = "Hypothesized Reading") + theme_bw()
g3
```
The following code graphs the change in ratings between experiments.
```{r}
g4 = ggplot(aes(y = (Implicature_Rate - Mean_Chance), x = ID), data = graph3_data) +
  geom_col(aes(fill = Hypothesized_Bias)) + 
  labs(title = 'Change in Means Between Tasks', x = 'Items', y = 'Inclusivity - Compatibility') +
  theme_bw() + theme(axis.text.x = element_blank())+ labs(fill = "Hypothesized Reading") +
  geom_hline(yintercept = 9.2, linetype = 2)+
  geom_hline(yintercept = -9.2, linetype = 2)
g4


```

# The following code creates a combined graph with all four previous graphs.
```{r}
gcombined = ggarrange(g, g2, g3, g4, ncol=1, nrow=4, common.legend = TRUE)

gcombined = ggarrange(g, g2, ncol=1, nrow=2, common.legend = TRUE)
gcombined

gcombined = ggarrange(g3, g4, ncol=1, nrow=2, common.legend = TRUE)
gcombined
```


```{r}
summary(lm(Implicature_Rate ~ Mean_Chance, data = graph3_data))
```



The following code graphs the means of incompatibility and implicature rates by item

The following model assesses the relationship between the items across each experiment. The fixed effect of each item reflects the model prediction for prior likelihood, while the fixed effect of experiment reflects the task effect. Lastly, the interaction between item and experiment reflects the change in rating attributable to "or". 
```{r}
data_part2$experiment = 1
data$experiment = 0


model3_data = rbind((data %>% select(ID, chance, experiment)), (data_part2%>% select(ID, chance, experiment)))

model_4 = lm(chance ~ ID * experiment, data = model3_data, contrasts = list(ID = contr.sum))
summary(model_4)

# Bayesian Version of Model 4 
model_5 = brm(chance ~ ID * experiment,
             data = model3_data,
             family = gaussian,
             iter = 4000,
             cores = 4,
             chains = 4,
             warmup = 1000,
             control = list(adapt_delta = 0.99, max_treedepth = 15),
             seed = 4
             )
print(model_5)
```

